{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vecCNNsentimentAnalysis",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mzalR3YocmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from keras.layers.core import Dense, SpatialDropout1D\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.pooling import GlobalMaxPooling1D\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "import collections\n",
        "import nltk\n",
        "import numpy as np\n",
        "import codecs\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQoKsStNrRBx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "144fb9aa-96d9-4861-ad36-f283cce523e7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJGOiqHupvpg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from __future__ import absolute_import\n",
        "from __future__ import unicode_literals\n",
        "from time import gmtime, strftime\n",
        "from keras.callbacks import TensorBoard\n",
        "import os\n",
        "\n",
        "\n",
        "def make_tensorboard(set_dir_name='',\n",
        "                     embeddings_freq=0,\n",
        "                     embeddings_layer_names=None,\n",
        "                     embeddings_metadata=None):\n",
        "    tictoc = strftime(\"%a_%d_%b_%Y_%H_%M_%S\", gmtime())\n",
        "    directory_name = tictoc\n",
        "    log_dir = set_dir_name + '_' + directory_name\n",
        "    os.mkdir(log_dir)\n",
        "    tensorboard = TensorBoard(log_dir=log_dir,\n",
        "                              embeddings_freq=embeddings_freq,\n",
        "                              embeddings_layer_names=embeddings_layer_names,\n",
        "                              embeddings_metadata=embeddings_metadata)\n",
        "    return tensorboard, log_dir"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jzjv3CppXzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from make_tensorboard import make_tensorboard"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuMwiksdqB6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "INPUT_FILE = \"drive/My Drive/training.txt\"\n",
        "# You have to get the model\n",
        "#    https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download\n",
        "WORD2VEC_MODEL = \"drive/My Drive/GoogleNews-vectors-negative300.bin.gz\"\n",
        "VOCAB_SIZE = 5000\n",
        "EMBED_SIZE = 300\n",
        "NUM_FILTERS = 256\n",
        "NUM_WORDS = 3\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 10"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub9YWQ7FqNA0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d37bf0fb-3248-4180-eb5c-56fdb9b8bbf5"
      },
      "source": [
        "counter = collections.Counter()\n",
        "fin = codecs.open(INPUT_FILE, \"r\", encoding='utf-8')\n",
        "maxlen = 0\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "for line in fin:\n",
        "    _, sent = line.strip().split(\"\\t\")\n",
        "    words = [x.lower() for x in nltk.word_tokenize(sent)]\n",
        "    if len(words) > maxlen:\n",
        "        maxlen = len(words)\n",
        "    for word in words:\n",
        "        counter[word] += 1\n",
        "fin.close()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_kEB7tSqXjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2index = collections.defaultdict(int)\n",
        "for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):\n",
        "    word2index[word[0]] = wid + 1\n",
        "vocab_sz = len(word2index) + 1\n",
        "index2word = {v: k for k, v in word2index.items()}\n",
        "\n",
        "xs, ys = [], []\n",
        "fin = codecs.open(INPUT_FILE, \"r\", encoding='utf-8')\n",
        "for line in fin:\n",
        "    label, sent = line.strip().split(\"\\t\")\n",
        "    ys.append(int(label))\n",
        "    words = [x.lower() for x in nltk.word_tokenize(sent)]\n",
        "    wids = [word2index[word] for word in words]\n",
        "    xs.append(wids)\n",
        "fin.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpKHvTW5qeoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = pad_sequences(xs, maxlen=maxlen)\n",
        "Y = np_utils.to_categorical(ys)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ2wfTkNqkY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a219a2a8-9fc9-4d2f-9bc7-ff869b7971ae"
      },
      "source": [
        "Xtrain, Xtest, Ytrain, Ytest = \\\n",
        "    train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4960, 42) (2126, 42) (4960, 2) (2126, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eycbZG-8qudj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d60ee665-1c04-47b1-d65e-23ad9a2f606f"
      },
      "source": [
        "# load word2vec model\n",
        "word2vec = KeyedVectors.load_word2vec_format(WORD2VEC_MODEL, binary=True)\n",
        "embedding_weights = np.zeros((vocab_sz, EMBED_SIZE))\n",
        "for word, index in word2index.items():\n",
        "    try:\n",
        "        embedding_weights[index, :] = word2vec[word]\n",
        "    except KeyError:\n",
        "        pass\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO-WQWfHq09V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "eb11769f-b878-4c56-c32b-554264530320"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_sz, EMBED_SIZE, input_length=maxlen,\n",
        "                    weights=[embedding_weights],\n",
        "                    trainable=True))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS,\n",
        "                 activation=\"relu\"))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(2, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "tensorboard, log_dir = make_tensorboard(\n",
        "    set_dir_name='keras_finetune_word2vec_embeddings')\n",
        "\n",
        "history = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,\n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    callbacks=[tensorboard],\n",
        "                    validation_data=(Xtest, Ytest))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " 1/78 [..............................] - ETA: 0s - loss: 0.7106 - accuracy: 0.5156WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            "78/78 [==============================] - 11s 135ms/step - loss: 0.1265 - accuracy: 0.9532 - val_loss: 0.0237 - val_accuracy: 0.9920\n",
            "Epoch 2/10\n",
            "78/78 [==============================] - 10s 124ms/step - loss: 0.0129 - accuracy: 0.9960 - val_loss: 0.0178 - val_accuracy: 0.9953\n",
            "Epoch 3/10\n",
            "78/78 [==============================] - 10s 123ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.0144 - val_accuracy: 0.9948\n",
            "Epoch 4/10\n",
            "78/78 [==============================] - 10s 129ms/step - loss: 0.0037 - accuracy: 0.9992 - val_loss: 0.0135 - val_accuracy: 0.9958\n",
            "Epoch 5/10\n",
            "78/78 [==============================] - 10s 122ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.0143 - val_accuracy: 0.9967\n",
            "Epoch 6/10\n",
            "78/78 [==============================] - 10s 125ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0137 - val_accuracy: 0.9962\n",
            "Epoch 7/10\n",
            "78/78 [==============================] - 10s 133ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0142 - val_accuracy: 0.9962\n",
            "Epoch 8/10\n",
            "78/78 [==============================] - 10s 122ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.0139 - val_accuracy: 0.9939\n",
            "Epoch 9/10\n",
            "78/78 [==============================] - 10s 123ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.0158 - val_accuracy: 0.9962\n",
            "Epoch 10/10\n",
            "78/78 [==============================] - 10s 131ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.0144 - val_accuracy: 0.9944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5qSmO3yq7v8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ad2c3b22-afd6-4d19-cfad-6b48363d3ec7"
      },
      "source": [
        "# evaluate model\n",
        "score = model.evaluate(Xtest, Ytest, verbose=1)\n",
        "print(\"Test score: {:.3f}, accuracy: {:.3f}\".format(score[0], score[1]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 1s 12ms/step - loss: 0.0144 - accuracy: 0.9944\n",
            "Test score: 0.014, accuracy: 0.994\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}